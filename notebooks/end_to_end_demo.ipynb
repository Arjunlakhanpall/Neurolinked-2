{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mind-to-Script â€” End-to-End Demo (Synthetic)\n",
    "\n",
    "This notebook demonstrates an end-to-end flow using synthetic EEG data:\n",
    "1. Create a canonical synthetic ZuCo-style `.pkl` file\n",
    "2. Build shards from canonical files (using the repository script)\n",
    "3. Create and save a bridge checkpoint (encoder + projection)\n",
    "4. Load the bridge into the runtime model with a dummy decoder/tokenizer\n",
    "5. Run inference on a synthetic EEG epoch\n",
    "\n",
    "This demo uses a lightweight dummy decoder to avoid downloading a large HF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path('..').resolve() / 'Neuralinked'\n",
    "BASE = Path('.').resolve()\n",
    "os.makedirs('data/canonical', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('data/shards', exist_ok=True)\n",
    "print('Working dir:', BASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Create a synthetic canonical pickle\n",
    "sfreq = 500.0\n",
    "n_channels = 8\n",
    "duration_s = 2.0\n",
    "n_samples = int(sfreq * duration_s)\n",
    "signals = np.random.randn(n_channels, n_samples).astype(np.float32)\n",
    "sentences = [\n",
    "    {\n",
    "        'text': 'hello world',\n",
    "        'words': ['hello', 'world'],\n",
    "        'onsets': [0.1, 1.0],\n",
    "        'offsets': [0.6, 1.5]\n",
    "    }\n",
    "]\n",
    "canonical = {\n",
    "    'signals': signals,\n",
    "    'sfreq': sfreq,\n",
    "    'ch_names': [f'EEG{i}' for i in range(n_channels)],\n",
    "    'sentences': sentences,\n",
    "    'meta': {'subject': 'subj_synth'}\n",
    "}\n",
    "p = Path('data/canonical/subj_synth_file1.pkl')\n",
    "with open(p, 'wb') as f:\n",
    "    pickle.dump(canonical, f)\n",
    "print('Wrote canonical pickle:', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Build shards using the repository script (invokes build_manifest_and_shards.py)\n",
    "import subprocess\n",
    "subprocess.run(['python', 'scripts/build_manifest_and_shards.py', '--canonical', 'data/canonical', '--out', 'data/shards', '--version', 'v0.0.1', '--shard-size', '16'], check=True)\n",
    "print('Shards built under data/shards/v0.0.1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Create and save a bridge checkpoint (encoder + projection)\n",
    "from app.model import EEGEncoder\n",
    "import torch\n",
    "cnn_channels = 32\n",
    "lstm_hidden = 64\n",
    "in_channels = n_channels\n",
    "encoder = EEGEncoder(in_channels=in_channels, cnn_channels=cnn_channels, lstm_hidden=lstm_hidden)\n",
    "projection = torch.nn.Linear(lstm_hidden*2, 128)  # dummy d_model=128 for demo\n",
    "ckpt = {\n",
    "    'encoder_state_dict': encoder.state_dict(),\n",
    "    'projection_state_dict': projection.state_dict(),\n",
    "    'config': {'in_channels': in_channels, 'cnn_channels': cnn_channels, 'lstm_hidden': lstm_hidden, 'hf_model_name': 'dummy'}\n",
    "}\n",
    "torch.save(ckpt, 'models/bridge.pt')\n",
    "print('Saved bridge checkpoint to models/bridge.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load the bridge into the runtime model and attach dummy tokenizer/decoder\n",
    "from app.model import MindToScriptModel\n",
    "import torch\n",
    "\n",
    "class DummyTokenizer:\n",
    "    def batch_decode(self, sequences, skip_special_tokens=True):\n",
    "        return ['decoded text'] * sequences.shape[0]\n",
    "\n",
    "class DummyDecoder:\n",
    "    def __init__(self, d_model=128, vocab_size=100):\n",
    "        self.config = type('C', (), {'d_model': d_model})\n",
    "        self.vocab_size = vocab_size\n",
    "    def generate(self, encoder_outputs=None, max_length=32, num_beams=1, return_dict_in_generate=True, output_scores=True):\n",
    "        batch = encoder_outputs.last_hidden_state.shape[0]\n",
    "        sequences = torch.randint(0, self.vocab_size, (batch, 5))\n",
    "        scores = [torch.randn(batch, self.vocab_size) for _ in range(4)]\n",
    "        return type('G', (), {'sequences': sequences, 'scores': scores})()\n",
    "\n",
    "m = MindToScriptModel(device='cpu')\n",
    "m.tokenizer = DummyTokenizer()\n",
    "m.decoder = DummyDecoder(d_model=128)\n",
    "# load bridge checkpoint manually\n",
    "ck = torch.load('models/bridge.pt', map_location='cpu')\n",
    "m._bridge_config = ck.get('config', {})\n",
    "m._bridge_ckpt = 'models/bridge.pt'\n",
    "m._ensure_encoder(in_channels)\n",
    "print('Model bridge loaded (encoder + projection).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Run inference on a synthetic epoch\n",
    "sig = np.random.randn(in_channels, 256).astype(float)\n",
    "texts, confidences = m.predict(sig)\n",
    "print('Predicted text:', texts[0])\n",
    "print('Confidence:', confidences[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now inspect `data/shards/v0.0.1/` for shard files and `models/bridge.pt` for the bridge checkpoint.\n",
    "Extend this notebook to run multiple synthetic examples, or replace the dummy decoder with a real HF model if you have network access."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
